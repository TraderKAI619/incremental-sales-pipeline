# incremental-sales-pipeline

**ğŸ”— Related Project**: [JP Retail Medallion Pipeline (Project A)](https://github.com/TraderKAI619/project-a-jp-retail-pipeline)
[![CI](https://github.com/TraderKAI619/incremental-sales-pipeline/actions/workflows/ci.yml/badge.svg)](https://github.com/TraderKAI619/incremental-sales-pipeline/actions/workflows/ci.yml)
Idempotent **incremental** sales pipeline with **8+ data-quality checks** (pandas + DuckDB).

## âš¡ Quick Start (3 steps)
```bash
# 1) Create & activate env, then install deps
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2) One-command run (ingest â†’ silver â†’ gold â†’ DQ â†’ demo)
make run

# 3) Run tests (idempotency + data-quality)
pytest -q -k "dq or idempotency"    # or: make check
```
## ğŸ¬ Demoï¼ˆ30ç§’ï¼‰
1) **GitHub Actions** â†’ æœ€æ–°ã® **Success** ã‚’é–‹ã  
2) **Artifacts** â†’ `dq-and-reports` ã‚’ã‚¯ãƒªãƒƒã‚¯  
3) ãƒ­ãƒ¼ã‚«ãƒ«å†ç¾ï¼š`make everything && pytest -q`

---

**ğŸ“š æ—¥æœ¬èªç‰ˆ README â†’** [README_ja.md](./README_ja.md)

---

## Data Quality (5 Layers)

Our pipeline implements comprehensive data quality checks across 5 categories:

| Category | What We Check | Implementation |
|----------|--------------|----------------|
| **Duplicates (é‡è¤‡)** | Natural key uniqueness: `order_date, geo_id, product_id` | `schemas/*.schema.json`, `scripts/validate_*.py`, `tests/` |
| **Missing (æ¬ æ)** | Required fields non-null/non-empty | Schema validation + quarantine logic |
| **Outliers (å¤–ã‚Œå€¤)** | Reasonable ranges for `quantity`, `unit_price`, `revenue_jpy` | Schema constraints + business rules |
| **Timezone (ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³)** | `order_date` normalized to JST (YYYYMMDD) | `scripts/generate_sales.py`, `scripts/to_silver.py` |
| **Schema (ã‚¹ã‚­ãƒ¼ãƒ)** | Column types, primary/foreign key compliance | `schemas/*.schema.json` validation |

## ğŸ—“ï¸ Operations / Schedule
- **Nightly**: **20:00 UTC = 05:00 JST** via GitHub Actions  
- **Artifacts**: `dq-and-reports`ï¼ˆ`dq_report.md` / `dq_dashboard.txt` / `fact_sales.csv`ï¼‰

md
## Quality Metrics
*Below values are sample metrics; for latest figures, see CI **Artifacts â†’ `dq-and-reports`**.*
- âœ… **Pass Rate**: **â‰ˆ95%ï¼ˆÂ±1â€“2ptï¼‰**ï¼ˆ**åˆ†æ¯ï¼šSilver**ï¼‰
- âš ï¸ **Quarantine**: **â‰ˆ5%**ï¼ˆç†ç”±ã¯ `dq_report.md` å‚ç…§ï¼‰
- ğŸ“Š **Gold Output**: ä¾‹ï¼‰**fact_sales ~310 rows**ã€returns ~11 rows
- ğŸ¯ **Alert Threshold**: quarantine rate < 25%

> æŒ‡æ¨™ã¯æ—¥æ¬¡ã§æ›´æ–°ã•ã‚Œã¾ã™ã€‚ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼å½“æ—¥å·®åˆ†ã«ã‚ˆã‚Š Â±1â€“2pt ã®è‡ªç„¶å¤‰å‹•ãŒã‚ã‚Šã¾ã™ï¼ˆ**åˆ†æ¯ï¼šSilver**ï¼‰ã€‚

**ğŸ“Š View Latest Reports (CI Artifacts):**  
Open the latest successful workflow run â†’ **Artifacts â†’ `dq-and-reports`**:
- `dq_report.md` â€” Silver + Gold validation summary
- `dq_dashboard.txt` â€” Comprehensive quality dashboard
- `fact_sales.csv` â€” Gold fact table (sample)


```mermaid
graph TD
  subgraph Silver
    B[Data Validation<br/>5 DQ Layers]
    C[Clean Data<br/>857 records]
    D[Quarantine<br/>43 records]
  end
  subgraph Gold
    E[fact_sales<br/>310 rows]
    F[fact_returns<br/>11 rows]
  end

  A[Raw CSV Files<br/>8 days data] -->|Ingest| B
  B -->|Pass 95.2%| C
  B -->|Fail 4.8%| D
  C -->|Aggregate| E
  D -->|Extract| F

```

## Design Notes & Provenance
- Decisions (ADR-lite): see DECISIONS.md
- Quarantine examples: see data/silver/quarantine/README.md

## Tooling & Authorship

I used AI assistants for **boilerplate and documentation polish** only.
**All pipeline logic (idempotent upserts), DQ rules (5 layers), schemas, tests, and CI gates are my own work.**
Metrics in this README are reproducible from this repoâ€™s **CI Artifacts** (see â€œView Latest Reportsâ€) and
from local files under `reports/` / `data/gold/`.

---

## ğŸ§ª Local checks
```bash
# Count rows (excluding headers)
awk 'NR>1' data/gold/fact_returns.csv | wc -l
awk 'NR>1' data/gold/fact_sales.csv   | wc -l
